<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>I, BayesOpt</title>
<meta name="author" content="(Gergely Flamich)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js/dist/theme/moon.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/npm/reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1>I, BayesOpt</h1><h2>Taking Humans Out of the Loop</h2></br><h4>Gergely Flamich</h4>
</section>

<section>
<section id="slide-org4fe18ab">
<h2 id="org4fe18ab"><span class="section-number-2">1</span> A Talk about Bayesian Optimization</h2>
<p>
Slides available at: <a href="https://rawcdn.githack.com/gergely-flamich/tmx-bayesopt-talk/ccf95ef72b9b1e586a3cd8a29bf5ece17a9efd99/presentation.html">https://rawcdn.githack.com/gergely-flamich/tmx-bayesopt-talk/ccf95ef72b9b1e586a3cd8a29bf5ece17a9efd99/presentation.html</a>
</p>
</section>
<section id="slide-org97e533f">
<h3 id="org97e533f"><span class="section-number-3">1.1</span> Motivation</h3>
<p>
Let's look at a few practical problems!
</p>
</section>
<section id="slide-orgc260b07">
<h4 id="orgc260b07"><span class="section-number-4">1.1.1</span> Aerodynamics</h4>

<div id="org89c49d2" class="figure">
<p><img src="./img/wing_params.png" alt="wing_params.png" class="fragment (appear)" />
</p>
</div>


<div id="orga719ee2" class="figure">
<p><img src="./img/banana_cars.png" alt="banana_cars.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgf1d690b">
<h4 id="orgf1d690b"><span class="section-number-4">1.1.2</span> Molecules</h4>

<div id="orgff38698" class="figure">
<p><img src="./img/drug_disc.png" alt="drug_disc.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org2cadfb9">
<h4 id="org2cadfb9"><span class="section-number-4">1.1.3</span> Hardware</h4>

<div id="org4346599" class="figure">
<p><img src="./img/raspi.jpg" alt="raspi.jpg" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org9b7f767">
<h4 id="org9b7f767"><span class="section-number-4">1.1.4</span> Training Machine Learning Algorithms</h4>

<div id="orgb700f15" class="figure">
<p><img src="./img/neural_network.png" alt="neural_network.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgf053292">
<h4 id="orgf053292"><span class="section-number-4">1.1.5</span> Why These Problems Are Hard</h4>
<ul>
<li class="fragment appear">Data is hard to obtain, e.g. takes long or very expensive</li>
<li class="fragment appear">Therefore data is scarce</li>
<li class="fragment appear">Design settings influence the outcome in complicated ways</li>
<li class="fragment appear">(Usually) no access to gradient information</li>

</ul>

</section>
<section id="slide-org311212d">
<h3 id="org311212d"><span class="section-number-3">1.2</span> The Optimization Problem, Abstractly</h3>
<div class="outline-text-3" id="text-1-2">
</div>
</section>
<section id="slide-orgc094181">
<h4 id="orgc094181"><span class="section-number-4">1.2.1</span> The Objective Function</h4>
<ul>
<li class="fragment appear"><b>Parameter space</b>: \(\mathcal{X}\)
<ul>
<li>wing design, reactants, ML hyperparameters</li>

</ul></li>
<li class="fragment appear"><b>Output space</b>: \(\mathbb{R}\)
<ul>
<li>drug potency, test accuracy</li>

</ul></li>
<li class="fragment appear"><b>Objective function</b>: \(f: \mathcal{X} \rightarrow \mathbb{R}\)
<ul>
<li>wind tunnel, chemical reaction, ML training algorithm</li>

</ul></li>

</ul>

<p class="fragment (appear)">
We assume that \(f\) is very expensive to evaluate.
</p>

</section>
<section id="slide-org0090cba">
<h4 id="org0090cba"><span class="section-number-4">1.2.2</span> The Optimization Problem</h4>
<p>
We want to find the parameter setting \(x^* \in \mathcal{X}\) that minimizes \(f\).
</p>

<p>
In other words, want to find \(x\) with smallest \(f(x)\):
</p>

<p>
\(x^* = argmin_{x\in\mathcal{X}} f(x)\)
</p>


</section>
<section id="slide-orgb52b2b8">
<h3 id="orgb52b2b8"><span class="section-number-3">1.3</span> Possible Solutions</h3>
<div class="outline-text-3" id="text-1-3">
</div>
</section>
<section id="slide-org8d787f7">
<h4 id="org8d787f7"><span class="section-number-4">1.3.1</span> Just try every possible parameter!</h4>
<p>
Evaluate \(f\) on every parameter in \(\mathcal{X}\), return best-performing one.
</p>
<ul>
<li class="fragment appear">Will find best solution</li>
<li class="fragment appear">Cannot be done if the parameter space is large</li>

</ul>

</section>
<section id="slide-orgfc76bbc">
<h4 id="orgfc76bbc"><span class="section-number-4">1.3.2</span> Randomly try possible parameters!</h4>
<p>
Sample \(S\) parameter settings from \(\mathcal{X}\), and evaluate \(f\) only on those.
</p>

<ul>
<li class="fragment appear">Will find approximate solution</li>
<li class="fragment appear">Approximation becomes better as \(S\) increases.</li>
<li class="fragment appear">Dumb (doesn't consider obtained information)</li>
<li class="fragment appear">Yet effective</li>

</ul>

</section>
<section id="slide-org8061e7d">
<h4 id="org8061e7d"><span class="section-number-4">1.3.3</span> Try to predict \(f\)!</h4>
<p class="fragment (appear)">
Maybe we already collected some data: \(\mathcal{D} = \{(x_1, f(x_1)), ... (x_N, f(x_N)) \}\).
</p>

<p class="fragment (appear)">
Then, we could <b>fit</b> maybe a <b>surrogate model</b> \(g\) that is cheap to evaluate.
</p>

<p class="fragment (appear)">
We can evaluate \(g\) on every parameter setting, and evaluate \(f\) at the best setting, as predicted by \(g\)!
</p>

</section>
<section id="slide-org4992707">
<h4 id="org4992707"><span class="section-number-4">1.3.4</span> Predict - Evaluate loop</h4>
<ol>
<li class="fragment appear">Fit \(g\) to already existing data \(\mathcal{D}\).</li>
<li class="fragment appear"><p>
Find <b>proposal</b> setting:
</p>

<p>
\(\hat{x} = argmin_{x \in \mathcal{X}} g(x)\)
</p></li>
<li class="fragment appear">Try proposal: \(f(\hat{x})\).</li>
<li class="fragment appear">Add \((\hat{x}, f(\hat{x}))\) to the dataset \(\mathcal{D}\).</li>

</ol>

<p class="fragment (appear)">
What \(g\) to use?
</p>

</section>
<section id="slide-orgfafa7b9">
<h4 id="orgfafa7b9"><span class="section-number-4">1.3.5</span> Fit a neural network!</h4>

<div id="org89c4725" class="figure">
<p><img src="./img/stack_more_layers.png" alt="stack_more_layers.png" />
</p>
</div>

</section>
<section id="slide-org5761bdd">
<h4 id="org5761bdd"><span class="section-number-4">1.3.6</span> Problems with a neural network</h4>

<div id="orge374c32" class="figure">
<p><img src="./img/nn_fit_to_little_data.png" alt="nn_fit_to_little_data.png" />
</p>
</div>

<ul>
<li class="fragment appear">Will overfit</li>
<li class="fragment appear">Will do weird stuff far away from the data</li>
<li class="fragment appear">How can we fix this?</li>

</ul>

<aside class="notes">
<p>
Enter speaker notes here.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org4fe23ac">
<h2 id="org4fe23ac"><span class="section-number-2">2</span> Gaussian Process Regression</h2>
<div class="outline-text-2" id="text-2">
</div>
</section>
<section id="slide-orga6091f2">
<h3 id="orga6091f2"><span class="section-number-3">2.1</span> Linear Regression</h3>
<p>
One model to rule them all
</p>

<p>
\(y = mx + b\)
</p>

</section>
<section id="slide-org53dd82d">
<h4 id="org53dd82d"><span class="section-number-4">2.1.1</span> Fitting to 2 points</h4>

<div id="orgbdce442" class="figure">
<p><img src="./img/two_point_lin_fit.png" alt="two_point_lin_fit.png" />
</p>
</div>

</section>
<section id="slide-orgcb3b031">
<h4 id="orgcb3b031"><span class="section-number-4">2.1.2</span> Fitting to more than 2 points</h4>

<div id="org468a34f" class="figure">
<p><img src="./img/lin_fit.png" alt="lin_fit.png" />
</p>
</div>

</section>
<section id="slide-org5680076">
<h4 id="org5680076"><span class="section-number-4">2.1.3</span> What if we have only 1 point???</h4>

</section>
<section id="slide-orga83f4a8">
<h3 id="orga83f4a8"><span class="section-number-3">2.2</span> Distributions over Functions</h3>
<div class="outline-text-3" id="text-2-2">
</div>
</section>
<section id="slide-orgca54376">
<h4 id="orgca54376"><span class="section-number-4">2.2.1</span> Bayesian Linear Regression</h4>
<p class="fragment (appear)">
Fitting to 1 data point is clearly impossible.
</p>

<p class="fragment (appear)">
We will introduce uncertainty about the model, using some <b>prior</b> belief.
</p>

<p class="fragment (appear)">
Then, given the <b>evidence</b> we update, and get our <b>posterior</b> belief.
</p>

<p class="fragment (appear)">
The posterior will contain models that are <b>consistent</b> with our data.
</p>

</section>
<section id="slide-orgcf9c4a2">
<h4 id="orgcf9c4a2"><span class="section-number-4">2.2.2</span> Bayes Rule</h4>
<p>
\(\overbrace{p(g \mid \mathcal{D})}^{\text{updated belief}} = \frac{\overbrace{p(\mathcal{D} \mid g)}^{\text{evidence}}\overbrace{p(g)}^{\text{earier belief}}}{\underbrace{p(\mathcal{D})}_{\text{"normalizing constant"}}}\)
</p>

</section>
<section id="slide-org939d74d">
<h4 id="org939d74d"><span class="section-number-4">2.2.3</span> Putting a Prior on Linear Regression</h4>
<p>
Assume, that the slope parameter is Gaussian distributed:
</p>

<p>
\(m \sim \mathcal{N}(0, 1), \quad b \sim \mathcal{N}(0, 1)\)
</p>

</section>
<section id="slide-orgf1696f2">
<h4 id="orgf1696f2"><span class="section-number-4">2.2.4</span> What the prior and posterior look like</h4>

<div id="org982de71" class="figure">
<p><img src="./img/bayes_lin_prior.png" alt="bayes_lin_prior.png" class="fragment (appear)" />
</p>
</div>


<div id="orge5e0955" class="figure">
<p><img src="./img/bayes_lin_post.png" alt="bayes_lin_post.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org0dde959">
<h3 id="org0dde959"><span class="section-number-3">2.3</span> Gaussian Processes</h3>
<p class="fragment (appear)">
For linear regression we put the prior on the <b>parameters</b>, which then specified the function.
</p>

<p class="fragment (appear)">
Gaussian Processes (GP) allow us to put the prior <b>straight on the function</b>.
</p>

<p class="fragment (appear)">
This is done through the use of <b>kernels functions</b> \(k(x, x')\).
</p>

<p class="fragment (appear)">
The kernel defines how the function's values \(f(x), f(x')\) are related to each other at the inputs \(x, x'\).
</p>

</section>
<section id="slide-orgd4149a5">
<h3 id="orgd4149a5"><span class="section-number-3">2.4</span> Different Kernels</h3>
<p class="fragment (appear)">
The kernel defines, what the properties that the samples obey. 
</p>

<p class="fragment (appear)">
This way we can bake <b>domain knowledge</b> into our model.
</p>

</section>
<section id="slide-orgcdc5558">
<h4 id="orgcdc5558"><span class="section-number-4">2.4.1</span> Linear Kernel</h4>
<p>
\(k_{lin}(x, x') = \alpha^2 (x \cdot x')\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>

</ul>


<div id="orgcf77f17" class="figure">
<p><img src="./img/bayes_lin_prior.png" alt="bayes_lin_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org73fd9f8">
<h4 id="org73fd9f8"><span class="section-number-4">2.4.2</span> Exponentiated Quadratic Kernel</h4>
<p>
\(k_{EQ}(x, x') = \alpha^2 \exp\left( -\frac{(x - x')^2}{2\ell^2} \right)\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>
<li>\(\ell\): length scale</li>

</ul>


<div id="orgd641d0b" class="figure">
<p><img src="./img/rbf_prior.png" alt="rbf_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org5f43b49">
<h4 id="org5f43b49"><span class="section-number-4">2.4.3</span> Periodic Kernel</h4>
<p>
\(k_{periodic}(x, x') = \alpha^2 \exp \left( -\frac{2\sin^2(\pi (x - x')^2 / p)}{\ell^2} \right)\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>
<li>\(\ell\): length scale</li>
<li>\(p\): period</li>

</ul>


<div id="org74ebcb0" class="figure">
<p><img src="./img/periodic_prior.png" alt="periodic_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgcfbe63f">
<h3 id="orgcfbe63f"><span class="section-number-3">2.5</span> Non-parametric Models</h3>
<p class="fragment (appear)">
GPs are a form of <b>non-parametric</b> model.
</p>

<p class="fragment (appear)">
In our setting, this means, that the <b>structure</b> of the model is not fixed ahead of time, and its complexity grows as more information becomes available.
</p>

</section>
<section id="slide-org0f0f311">
<h3 id="org0f0f311"><span class="section-number-3">2.6</span> Prediction using Gaussian processes</h3>
<p>
Given a new input point \(x^*\), GPs allow us to form the <b>predictive posterior</b>: \(p(x^* \mid \mathcal{Data})\).
<img src="./img/post_rbf_gp.png" alt="post_rbf_gp.png" />
</p>

</section>
<section id="slide-orgeab15ae">
<h3 id="orgeab15ae"><span class="section-number-3">2.7</span> Tuning the Hyperparameters</h3>
<p class="fragment (appear)">
Even though the model will accomodate the data, we would like the <b>model family</b> that fits the data best.
</p>

<p class="fragment (appear)">
Hence, we must tune the <b>hyperparameters</b> of the model.
</p>

<p class="fragment (appear)">
Luckily for GPs, there is a very natural way: we <b>maximize</b> the <b>prior</b> likelihood of the dataset:
\(p(\mathcal{D})\).
</p>

</section>
<section id="slide-orgbad84a2">
<h4 id="orgbad84a2"><span class="section-number-4">2.7.1</span> Small example</h4>

<div id="org404512f" class="figure">
<p><img src="./img/pre-vs-post-optim.png" alt="pre-vs-post-optim.png" />
</p>
</div>

</section>
<section id="slide-org5e53366">
<h3 id="org5e53366"><span class="section-number-3">2.8</span> Advantages of Gaussian Processes</h3>
<ul>
<li class="fragment appear">Interpretable</li>
<li class="fragment appear">Very sample-efficient</li>
<li class="fragment appear">Very flexible</li>
<li class="fragment appear">Strong mathematical maturity</li>

</ul>

</section>
<section id="slide-orgab169e9">
<h3 id="orgab169e9"><span class="section-number-3">2.9</span> Drawbacks of Gaussian Processes</h3>
<ul>
<li class="fragment appear">Both training and prediction grow as \(\mathcal{O}(N^3)\), where \(N\) is the size of the dataset.</li>
<li class="fragment appear">Hard to do multi-output prediction (though not impossible)</li>
<li class="fragment appear">Finding the right kernel can be tricky for more complicated problems, especially in high dimensions</li>

</ul>

</section>
</section>
<section>
<section id="slide-org8c41d02">
<h2 id="org8c41d02"><span class="section-number-2">3</span> Bayesian Optimization</h2>
<div class="outline-text-2" id="text-3">
</div>
</section>
<section id="slide-orgc5c9dfa">
<h3 id="orgc5c9dfa"><span class="section-number-3">3.1</span> The Optimization Problem, Revisited</h3>
<ul>
<li class="fragment appear">We wish to minimize the objective function \(f\).</li>
<li class="fragment appear">Want to incorporate already collected information: \(\mathcal{D} = \{(x_1, f(x_1)), ... (x_N, f(x_N)) \}\).</li>
<li class="fragment appear">Want to use a sample-efficient surrogate model</li>
<li class="fragment appear">GPs are a perfect match!</li>

</ul>

</section>
<section id="slide-org6d32975">
<h3 id="org6d32975"><span class="section-number-3">3.2</span> The final ingredient: Acquisition functions</h3>
<p class="fragment (appear)">
<b>Acquisition functions</b> tell us potentially how good a solution is.
</p>

<p class="fragment (appear)">
For us, acquistion functions will be the means through which we can <b>incorporate uncertainty</b> into the selection.
</p>

</section>
<section id="slide-orga0ba3fe">
<h4 id="orga0ba3fe"><span class="section-number-4">3.2.1</span> The upper confidence bound</h4>
<p>
We will use the <b>upper confidence bound</b>:
</p>

<p>
\(\alpha(x) = \mu(x) + 2\sigma(x)\)
</p>

<p>
where \(\mu(x)\) is the <b>predictive mean</b> of the GP and \(\sigma(x)\) is the <b>predictive standard deviation</b>.
</p>

</section>
<section id="slide-orgd91dee7">
<h3 id="orgd91dee7"><span class="section-number-3">3.3</span> Predict - Evaluate loop, Revisited</h3>
<p>
We pick our surrogate model to be a GP.
</p>

<p>
Then, the optimization loop becomes:
</p>
<ol>
<li class="fragment appear">Fit the GP to already existing data \(\mathcal{D}\).</li>
<li class="fragment appear"><p>
Find <b>proposal</b> setting:
</p>

<p>
\(\hat{x} = argmin_{x \in \mathcal{X}} \alpha(x)\)
</p></li>
<li class="fragment appear">Try proposal: \(f(\hat{x})\).</li>
<li class="fragment appear">Add \((\hat{x}, f(\hat{x}))\) to the dataset \(\mathcal{D}\).</li>
<li class="fragment appear">Repeat until satisfaction.</li>

</ol>

</section>
<section id="slide-org8f2f65c">
<h3 id="org8f2f65c"><span class="section-number-3">3.4</span> A Toy Example</h3>
<p>
We are going to optimize the toy function
</p>

<p>
\(f(x) = \frac{sin(x)}{x} + \frac{1}{2}\left(x - \frac{1}{2}\right)^2\)
</p>

</section>
<section id="slide-org7df2ab3">
<h4 id="org7df2ab3"><span class="section-number-4">3.4.1</span> Toy Example</h4>

<div id="orgc1b5c61" class="figure">
<p><img src="./img/toy_example/gp_manual_0.png" alt="gp_manual_0.png" />
</p>
</div>
</section>
<section id="slide-org4d2f8c3">
<h4 id="org4d2f8c3"><span class="section-number-4">3.4.2</span> Toy Example</h4>

<div id="org555fcf8" class="figure">
<p><img src="./img/toy_example/gp_manual_1.png" alt="gp_manual_1.png" />
</p>
</div>
</section>
<section id="slide-org9aba485">
<h4 id="org9aba485"><span class="section-number-4">3.4.3</span> Toy Example</h4>

<div id="orge58ccde" class="figure">
<p><img src="./img/toy_example/gp_manual_2.png" alt="gp_manual_2.png" />
</p>
</div>
</section>
<section id="slide-orgf7267f0">
<h4 id="orgf7267f0"><span class="section-number-4">3.4.4</span> Toy Example</h4>

<div id="org628c9aa" class="figure">
<p><img src="./img/toy_example/gp_manual_3.png" alt="gp_manual_3.png" />
</p>
</div>
</section>
<section id="slide-org3e9ebfe">
<h4 id="org3e9ebfe"><span class="section-number-4">3.4.5</span> Toy Example</h4>

<div id="org5e27c86" class="figure">
<p><img src="./img/toy_example/gp_manual_4.png" alt="gp_manual_4.png" />
</p>
</div>


</section>
<section id="slide-org0024532">
<h3 id="org0024532"><span class="section-number-3">3.5</span> A More Involved Example</h3>
</section>
</section>
<section>
<section id="slide-orgb805cf4">
<h2 id="orgb805cf4"><span class="section-number-2">4</span> References</h2>
<div class="outline-text-2" id="text-4">
</div>
</section>
<section id="slide-org6af9639">
<h3 id="org6af9639"><span class="section-number-3">4.1</span> References I</h3>
<ul>
<li><a href="https://www.theguardian.com/society/2020/feb/20/antibiotic-that-kills-drug-resistant-bacteria-discovered-through-ai">https://www.theguardian.com/society/2020/feb/20/antibiotic-that-kills-drug-resistant-bacteria-discovered-through-ai</a></li>
<li><a href="https://en.wikipedia.org/wiki/Raspberry_Pi">https://en.wikipedia.org/wiki/Raspberry_Pi</a></li>
<li>Gaier, Adam, Alexander Asteroth, and Jean-Baptiste Mouret. "Aerodynamic design exploration through surrogate-assisted illumination." 18th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference. 2017.</li>

</ul>
</section>
<section id="slide-org33df0ac">
<h3 id="org33df0ac"><span class="section-number-3">4.2</span> References II</h3>
<ul>
<li>GÃ³mez-Bombarelli, Rafael, et al. "Automatic chemical design using a data-driven continuous representation of molecules." ACS central science 4.2 (2018): 268-276</li>
<li>Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. "Practical bayesian optimization of machine learning algorithms." arXiv preprint arXiv:1206.2944 (2012).</li>
<li>Shahriari, Bobak, et al. "Taking the human out of the loop: A review of Bayesian optimization." Proceedings of the IEEE 104.1 (2015): 148-175.</li>

</ul>
</section>
<section id="slide-org82deddc">
<h3 id="org82deddc"><span class="section-number-3">4.3</span> References III</h3>
<ul>
<li>Srinivas, Niranjan, et al. "Gaussian process optimization in the bandit setting: No regret and experimental design." arXiv preprint arXiv:0912.3995 (2009).</li>
<li>Garnett, Roman, Michael A. Osborne, and Stephen J. Roberts. "Bayesian optimization for sensor set selection." Proceedings of the 9th ACM/IEEE international conference on information processing in sensor networks. 2010.</li>

</ul>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/dist/reveal.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/markdown/markdown.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/zoom/zoom.js"></script>
<script src="https://cdn.jsdelivr.net/npm/reveal.js/plugin/notes/notes.js"></script>


<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
plugins: [RevealMarkdown,RevealZoom,RevealNotes],
slideNumber:'c/t',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
]
});


</script>
</body>
</html>
