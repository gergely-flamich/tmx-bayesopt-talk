<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>I, BayesOpt</title>
<meta name="author" content="(Gergely Flamich)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/dist/reveal.css"/>

<link rel="stylesheet" href="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/dist/theme/moon.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1>I, BayesOpt</h1><h2>Taking Humans Out of the Loop</h2></br><h4>Gergely Flamich</h4>
</section>

<section>
<section id="slide-orgb4a9183">
<h2 id="orgb4a9183"><span class="section-number-2">1</span> A Talk about Bayesian Optimization</h2>
<div class="outline-text-2" id="text-1">
</div>
</section>
<section id="slide-org961d9d6">
<h3 id="org961d9d6"><span class="section-number-3">1.1</span> Motivation</h3>
<p>
Let's look at a few practical problems!
</p>
</section>
<section id="slide-org3444302">
<h4 id="org3444302"><span class="section-number-4">1.1.1</span> Aerodynamics</h4>

<div id="org4b6fdcb" class="figure">
<p><img src="./img/wing_params.png" alt="wing_params.png" class="fragment (appear)" />
</p>
</div>


<div id="org5340883" class="figure">
<p><img src="./img/banana_cars.png" alt="banana_cars.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgcf7ff3f">
<h4 id="orgcf7ff3f"><span class="section-number-4">1.1.2</span> Molecules</h4>

<div id="org327e408" class="figure">
<p><img src="./img/drug_disc.png" alt="drug_disc.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgb1381f0">
<h4 id="orgb1381f0"><span class="section-number-4">1.1.3</span> Hardware</h4>

<div id="org80fa11e" class="figure">
<p><img src="./img/raspi.jpg" alt="raspi.jpg" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org10d08a3">
<h4 id="org10d08a3"><span class="section-number-4">1.1.4</span> Training Machine Learning Algorithms</h4>

<div id="org7c0f289" class="figure">
<p><img src="./img/neural_network.png" alt="neural_network.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgd5cfabe">
<h4 id="orgd5cfabe"><span class="section-number-4">1.1.5</span> Why These Problems Are Hard</h4>
<ul>
<li class="fragment appear">Data is hard to obtain, e.g. takes long or very expensive</li>
<li class="fragment appear">Therefore data is scarce</li>
<li class="fragment appear">Design settings influence the outcome in complicated ways</li>
<li class="fragment appear">(Usually) no access to gradient information</li>

</ul>

</section>
<section id="slide-orge6f8e0b">
<h3 id="orge6f8e0b"><span class="section-number-3">1.2</span> The Optimization Problem, Abstractly</h3>
<div class="outline-text-3" id="text-1-2">
</div>
</section>
<section id="slide-org63e08df">
<h4 id="org63e08df"><span class="section-number-4">1.2.1</span> The Objective Function</h4>
<ul>
<li class="fragment appear"><b>Parameter space</b>: \(\mathcal{X}\)
<ul>
<li>wing design, reactants, ML hyperparameters</li>

</ul></li>
<li class="fragment appear"><b>Output space</b>: \(\mathbb{R}\)
<ul>
<li>drug potency, test accuracy</li>

</ul></li>
<li class="fragment appear"><b>Objective function</b>: \(f: \mathcal{X} \rightarrow \mathbb{R}\)
<ul>
<li>wind tunnel, chemical reaction, ML training algorithm</li>

</ul></li>

</ul>

<p class="fragment (appear)">
We assume that \(f\) is very expensive to evaluate.
</p>

</section>
<section id="slide-orgd279abb">
<h4 id="orgd279abb"><span class="section-number-4">1.2.2</span> The Optimization Problem</h4>
<p>
We want to find the parameter setting \(x^* \in \mathcal{X}\) that minimizes \(f\).
</p>

<p>
In other words, want to find \(x\) with smallest \(f(x)\):
</p>

<p>
\(x^* = argmin_{x\in\mathcal{X}} f(x)\)
</p>


</section>
<section id="slide-orgd870253">
<h3 id="orgd870253"><span class="section-number-3">1.3</span> Possible Solutions</h3>
<div class="outline-text-3" id="text-1-3">
</div>
</section>
<section id="slide-org7df423e">
<h4 id="org7df423e"><span class="section-number-4">1.3.1</span> Just try every possible parameter!</h4>
<p>
Evaluate \(f\) on every parameter in \(\mathcal{X}\), return best-performing one.
</p>
<ul>
<li class="fragment appear">Will find best solution</li>
<li class="fragment appear">Cannot be done if the parameter space is large</li>

</ul>

</section>
<section id="slide-org3c4aa9b">
<h4 id="org3c4aa9b"><span class="section-number-4">1.3.2</span> Randomly try possible parameters!</h4>
<p>
Sample \(S\) parameter settings from \(\mathcal{X}\), and evaluate \(f\) only on those.
</p>

<ul>
<li class="fragment appear">Will find approximate solution</li>
<li class="fragment appear">Approximation becomes better as \(S\) increases.</li>
<li class="fragment appear">Dumb (doesn't consider obtained information)</li>
<li class="fragment appear">Yet effective</li>

</ul>

</section>
<section id="slide-orgf092db9">
<h4 id="orgf092db9"><span class="section-number-4">1.3.3</span> Try to predict \(f\)!</h4>
<p class="fragment (appear)">
Maybe we already collected some data: \(\mathcal{D} = \{(x_1, f(x_1)), ... (x_N, f(x_N)) \}\).
</p>

<p class="fragment (appear)">
Then, we could <b>fit</b> maybe a <b>surrogate model</b> \(g\) that is cheap to evaluate.
</p>

<p class="fragment (appear)">
We can evaluate \(g\) on every parameter setting, and evaluate \(f\) at the best setting, as predicted by \(g\)!
</p>

</section>
<section id="slide-org620747e">
<h4 id="org620747e"><span class="section-number-4">1.3.4</span> Predict - Evaluate loop</h4>
<ol>
<li class="fragment appear">Fit \(g\) to already existing data \(\mathcal{D}\).</li>
<li class="fragment appear"><p>
Find <b>proposal</b> setting:
</p>

<p>
\(\hat{x} = argmin_{x \in \mathcal{X}} g(x)\)
</p></li>
<li class="fragment appear">Try proposal: \(f(\hat{x})\).</li>
<li class="fragment appear">Add \((\hat{x}, f(\hat{x}))\) to the dataset \(\mathcal{D}\).</li>

</ol>

<p class="fragment (appear)">
What \(g\) to use?
</p>

</section>
<section id="slide-orgeff7e66">
<h4 id="orgeff7e66"><span class="section-number-4">1.3.5</span> Fit a neural network!</h4>

<div id="orgf03f08d" class="figure">
<p><img src="./img/stack_more_layers.png" alt="stack_more_layers.png" />
</p>
</div>

</section>
<section id="slide-orgbcde6e0">
<h4 id="orgbcde6e0"><span class="section-number-4">1.3.6</span> Problems with a neural network</h4>

<div id="org53fe51d" class="figure">
<p><img src="./img/nn_fit_to_little_data.png" alt="nn_fit_to_little_data.png" />
</p>
</div>

<ul>
<li class="fragment appear">Will overfit</li>
<li class="fragment appear">Will do weird stuff far away from the data</li>
<li class="fragment appear">How can we fix this?</li>

</ul>

<aside class="notes">
<p>
Enter speaker notes here.
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org806e3f9">
<h2 id="org806e3f9"><span class="section-number-2">2</span> Gaussian Process Regression</h2>
<div class="outline-text-2" id="text-2">
</div>
</section>
<section id="slide-orgb414f5a">
<h3 id="orgb414f5a"><span class="section-number-3">2.1</span> Linear Regression</h3>
<p>
One model to rule them all
</p>

<p>
\(y = mx + b\)
</p>

</section>
<section id="slide-org9ecb4b6">
<h4 id="org9ecb4b6"><span class="section-number-4">2.1.1</span> Fitting to 2 points</h4>

<div id="orgf90c331" class="figure">
<p><img src="./img/two_point_lin_fit.png" alt="two_point_lin_fit.png" />
</p>
</div>

</section>
<section id="slide-org8b0b898">
<h4 id="org8b0b898"><span class="section-number-4">2.1.2</span> Fitting to more than 2 points</h4>

<div id="org6ea7b1b" class="figure">
<p><img src="./img/lin_fit.png" alt="lin_fit.png" />
</p>
</div>

</section>
<section id="slide-orgc533f27">
<h4 id="orgc533f27"><span class="section-number-4">2.1.3</span> What if we have only 1 point???</h4>

</section>
<section id="slide-orgb37757c">
<h3 id="orgb37757c"><span class="section-number-3">2.2</span> Distributions over Functions</h3>
<div class="outline-text-3" id="text-2-2">
</div>
</section>
<section id="slide-org783d426">
<h4 id="org783d426"><span class="section-number-4">2.2.1</span> Bayesian Linear Regression</h4>
<p class="fragment (appear)">
Fitting to 1 data point is clearly impossible.
</p>

<p class="fragment (appear)">
We will introduce uncertainty about the model, using some <b>prior</b> belief.
</p>

<p class="fragment (appear)">
Then, given the <b>evidence</b> we update, and get our <b>posterior</b> belief.
</p>

<p class="fragment (appear)">
The posterior will contain models that are <b>consistent</b> with our data.
</p>

</section>
<section id="slide-org558300e">
<h4 id="org558300e"><span class="section-number-4">2.2.2</span> Bayes Rule</h4>
<p>
\(\overbrace{p(g \mid \mathcal{D})}^{\text{updated belief}} = \frac{\overbrace{p(\mathcal{D} \mid g)}^{\text{evidence}}\overbrace{p(g)}^{\text{earier belief}}}{\underbrace{p(\mathcal{D})}_{\text{"normalizing constant"}}}\)
</p>

</section>
<section id="slide-org17c17e2">
<h4 id="org17c17e2"><span class="section-number-4">2.2.3</span> Putting a Prior on Linear Regression</h4>
<p>
Assume, that the slope parameter is Gaussian distributed:
</p>

<p>
\(m \sim \mathcal{N}(0, 1), \quad b \sim \mathcal{N}(0, 1)\)
</p>

</section>
<section id="slide-org66c65c8">
<h4 id="org66c65c8"><span class="section-number-4">2.2.4</span> What the prior and posterior look like</h4>

<div id="org6d83855" class="figure">
<p><img src="./img/bayes_lin_prior.png" alt="bayes_lin_prior.png" class="fragment (appear)" />
</p>
</div>


<div id="org99fcc59" class="figure">
<p><img src="./img/bayes_lin_post.png" alt="bayes_lin_post.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgde4bd87">
<h3 id="orgde4bd87"><span class="section-number-3">2.3</span> Gaussian Processes</h3>
<p class="fragment (appear)">
For linear regression we put the prior on the <b>parameters</b>, which then specified the function.
</p>

<p class="fragment (appear)">
Gaussian Processes (GP) allow us to put the prior <b>straight on the function</b>.
</p>

<p class="fragment (appear)">
This is done through the use of <b>kernels functions</b> \(k(x, x')\).
</p>

<p class="fragment (appear)">
The kernel defines how the function's values \(f(x), f(x')\) are related to each other at the inputs \(x, x'\).
</p>

</section>
<section id="slide-org6defe2c">
<h3 id="org6defe2c"><span class="section-number-3">2.4</span> Different Kernels</h3>
<p class="fragment (appear)">
The kernel defines, what the properties that the samples obey. 
</p>

<p class="fragment (appear)">
This way we can bake <b>domain knowledge</b> into our model.
</p>

</section>
<section id="slide-org1166fee">
<h4 id="org1166fee"><span class="section-number-4">2.4.1</span> Linear Kernel</h4>
<p>
\(k_{lin}(x, x') = \alpha^2 (x \cdot x')\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>

</ul>


<div id="org7757015" class="figure">
<p><img src="./img/bayes_lin_prior.png" alt="bayes_lin_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-orgdede120">
<h4 id="orgdede120"><span class="section-number-4">2.4.2</span> Exponentiated Quadratic Kernel</h4>
<p>
\(k_{EQ}(x, x') = \alpha^2 \exp\left( -\frac{(x - x')^2}{2\ell^2} \right)\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>
<li>\(\ell\): length scale</li>

</ul>


<div id="org4b6586e" class="figure">
<p><img src="./img/rbf_prior.png" alt="rbf_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org4447947">
<h4 id="org4447947"><span class="section-number-4">2.4.3</span> Periodic Kernel</h4>
<p>
\(k_{periodic}(x, x') = \alpha^2 \exp \left( -\frac{2\sin^2(\pi (x - x')^2 / p)}{\ell^2} \right)\)
</p>

<ul>
<li>\(\alpha^2\): variance</li>
<li>\(\ell\): length scale</li>
<li>\(p\): period</li>

</ul>


<div id="orgbe50aa4" class="figure">
<p><img src="./img/periodic_prior.png" alt="periodic_prior.png" class="fragment (appear)" />
</p>
</div>

</section>
<section id="slide-org0b00f75">
<h3 id="org0b00f75"><span class="section-number-3">2.5</span> Non-parametric Models</h3>
<p class="fragment (appear)">
GPs are a form of <b>non-parametric</b> model.
</p>

<p class="fragment (appear)">
In our setting, this means, that the <b>structure</b> of the model is not fixed ahead of time, and its complexity grows as more information becomes available.
</p>

</section>
<section id="slide-orgb8ac7ce">
<h3 id="orgb8ac7ce"><span class="section-number-3">2.6</span> Prediction using Gaussian processes</h3>
<p>
Given a new input point \(x^*\), GPs allow us to form the <b>predictive posterior</b>: \(p(x^* \mid \mathcal{Data})\).
<img src="./img/post_rbf_gp.png" alt="post_rbf_gp.png" />
</p>

</section>
<section id="slide-orgf11c70a">
<h3 id="orgf11c70a"><span class="section-number-3">2.7</span> Tuning the Hyperparameters</h3>
<p class="fragment (appear)">
Even though the model will accomodate the data, we would like the <b>model family</b> that fits the data best.
</p>

<p class="fragment (appear)">
Hence, we must tune the <b>hyperparameters</b> of the model.
</p>

<p class="fragment (appear)">
Luckily for GPs, there is a very natural way: we <b>maximize</b> the <b>prior</b> likelihood of the dataset:
\(p(\mathcal{D})\).
</p>

</section>
<section id="slide-org0401bab">
<h4 id="org0401bab"><span class="section-number-4">2.7.1</span> Small example</h4>

<div id="org8e07b67" class="figure">
<p><img src="./img/pre-vs-post-optim.png" alt="pre-vs-post-optim.png" />
</p>
</div>

</section>
<section id="slide-org825673b">
<h3 id="org825673b"><span class="section-number-3">2.8</span> Advantages of Gaussian Processes</h3>
<ul>
<li class="fragment appear">Interpretable</li>
<li class="fragment appear">Very sample-efficient</li>
<li class="fragment appear">Very flexible</li>
<li class="fragment appear">Strong mathematical maturity</li>

</ul>

</section>
<section id="slide-org6bfc3ff">
<h3 id="org6bfc3ff"><span class="section-number-3">2.9</span> Drawbacks of Gaussian Processes</h3>
<ul>
<li class="fragment appear">Both training and prediction grow as \(\mathcal{O}(N^3)\), where \(N\) is the size of the dataset.</li>
<li class="fragment appear">Hard to do multi-output prediction (though not impossible)</li>
<li class="fragment appear">Finding the right kernel can be tricky for more complicated problems, especially in high dimensions</li>

</ul>

</section>
</section>
<section>
<section id="slide-org7c425ba">
<h2 id="org7c425ba"><span class="section-number-2">3</span> Bayesian Optimization</h2>
<div class="outline-text-2" id="text-3">
</div>
</section>
<section id="slide-orgdcd45e0">
<h3 id="orgdcd45e0"><span class="section-number-3">3.1</span> The Optimization Problem, Revisited</h3>
<ul>
<li class="fragment appear">We wish to minimize the objective function \(f\).</li>
<li class="fragment appear">Want to incorporate already collected information: \(\mathcal{D} = \{(x_1, f(x_1)), ... (x_N, f(x_N)) \}\).</li>
<li class="fragment appear">Want to use a sample-efficient surrogate model</li>
<li class="fragment appear">GPs are a perfect match!</li>

</ul>

</section>
<section id="slide-org9aa81d0">
<h3 id="org9aa81d0"><span class="section-number-3">3.2</span> The final ingredient: Acquisition functions</h3>
<p class="fragment (appear)">
<b>Acquisition functions</b> tell us potentially how good a solution is.
</p>

<p class="fragment (appear)">
For us, acquistion functions will be the means through which we can <b>incorporate uncertainty</b> into the selection.
</p>

</section>
<section id="slide-org99379d5">
<h4 id="org99379d5"><span class="section-number-4">3.2.1</span> The upper confidence bound</h4>
<p>
We will use the <b>upper confidence bound</b>:
</p>

<p>
\(\alpha(x) = \mu(x) + 2\sigma(x)\)
</p>

<p>
where \(\mu(x)\) is the <b>predictive mean</b> of the GP and \(\sigma(x)\) is the <b>predictive standard deviation</b>.
</p>

</section>
<section id="slide-org34ecc33">
<h3 id="org34ecc33"><span class="section-number-3">3.3</span> Predict - Evaluate loop, Revisited</h3>
<p>
We pick our surrogate model to be a GP.
</p>

<p>
Then, the optimization loop becomes:
</p>
<ol>
<li class="fragment appear">Fit the GP to already existing data \(\mathcal{D}\).</li>
<li class="fragment appear"><p>
Find <b>proposal</b> setting:
</p>

<p>
\(\hat{x} = argmin_{x \in \mathcal{X}} \alpha(x)\)
</p></li>
<li class="fragment appear">Try proposal: \(f(\hat{x})\).</li>
<li class="fragment appear">Add \((\hat{x}, f(\hat{x}))\) to the dataset \(\mathcal{D}\).</li>
<li class="fragment appear">Repeat until satisfaction.</li>

</ol>

</section>
<section id="slide-org684536d">
<h3 id="org684536d"><span class="section-number-3">3.4</span> A Toy Example</h3>
<p>
We are going to optimize the toy function
</p>

<p>
\(f(x) = \frac{sin(x)}{x} + \frac{1}{2}\left(x - \frac{1}{2}\right)^2\)
</p>

</section>
<section id="slide-org99c4aff">
<h4 id="org99c4aff"><span class="section-number-4">3.4.1</span> Toy Example</h4>

<div id="org54db766" class="figure">
<p><img src="./img/toy_example/gp_manual_0.png" alt="gp_manual_0.png" />
</p>
</div>
</section>
<section id="slide-org43dfaee">
<h4 id="org43dfaee"><span class="section-number-4">3.4.2</span> Toy Example</h4>

<div id="org0eedafc" class="figure">
<p><img src="./img/toy_example/gp_manual_1.png" alt="gp_manual_1.png" />
</p>
</div>
</section>
<section id="slide-orgfcb895a">
<h4 id="orgfcb895a"><span class="section-number-4">3.4.3</span> Toy Example</h4>

<div id="orgedc4ad0" class="figure">
<p><img src="./img/toy_example/gp_manual_2.png" alt="gp_manual_2.png" />
</p>
</div>
</section>
<section id="slide-orga35c066">
<h4 id="orga35c066"><span class="section-number-4">3.4.4</span> Toy Example</h4>

<div id="orga5528c3" class="figure">
<p><img src="./img/toy_example/gp_manual_3.png" alt="gp_manual_3.png" />
</p>
</div>
</section>
<section id="slide-orgaf4788d">
<h4 id="orgaf4788d"><span class="section-number-4">3.4.5</span> Toy Example</h4>

<div id="orged3f854" class="figure">
<p><img src="./img/toy_example/gp_manual_4.png" alt="gp_manual_4.png" />
</p>
</div>


</section>
<section id="slide-orgb30972b">
<h3 id="orgb30972b"><span class="section-number-3">3.5</span> A More Involved Example</h3>
</section>
</section>
<section>
<section id="slide-org6204482">
<h2 id="org6204482"><span class="section-number-2">4</span> References</h2>
<div class="outline-text-2" id="text-4">
</div>
</section>
<section id="slide-org629dec3">
<h3 id="org629dec3"><span class="section-number-3">4.1</span> References I</h3>
<ul>
<li><a href="https://www.theguardian.com/society/2020/feb/20/antibiotic-that-kills-drug-resistant-bacteria-discovered-through-ai">https://www.theguardian.com/society/2020/feb/20/antibiotic-that-kills-drug-resistant-bacteria-discovered-through-ai</a></li>
<li><a href="https://en.wikipedia.org/wiki/Raspberry_Pi">https://en.wikipedia.org/wiki/Raspberry_Pi</a></li>
<li>Gaier, Adam, Alexander Asteroth, and Jean-Baptiste Mouret. "Aerodynamic design exploration through surrogate-assisted illumination." 18th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference. 2017.</li>

</ul>
</section>
<section id="slide-org00c41ee">
<h3 id="org00c41ee"><span class="section-number-3">4.2</span> References II</h3>
<ul>
<li>GÃ³mez-Bombarelli, Rafael, et al. "Automatic chemical design using a data-driven continuous representation of molecules." ACS central science 4.2 (2018): 268-276</li>
<li>Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. "Practical bayesian optimization of machine learning algorithms." arXiv preprint arXiv:1206.2944 (2012).</li>
<li>Shahriari, Bobak, et al. "Taking the human out of the loop: A review of Bayesian optimization." Proceedings of the IEEE 104.1 (2015): 148-175.</li>

</ul>
</section>
<section id="slide-org8cc025b">
<h3 id="org8cc025b"><span class="section-number-3">4.3</span> References III</h3>
<ul>
<li>Srinivas, Niranjan, et al. "Gaussian process optimization in the bandit setting: No regret and experimental design." arXiv preprint arXiv:0912.3995 (2009).</li>
<li>Garnett, Roman, Michael A. Osborne, and Stephen J. Roberts. "Bayesian optimization for sensor set selection." Proceedings of the 9th ACM/IEEE international conference on information processing in sensor networks. 2010.</li>

</ul>
</section>
</section>
</div>
</div>
<script src="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/dist/reveal.js"></script>
<script src="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/plugin/markdown/markdown.js"></script>
<script src="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/plugin/zoom/zoom.js"></script>
<script src="file:///Users/gergelyflamich/Documents/PhD/presentations/TMX_2021_BayesOpt/reveal.js-master/plugin/notes/notes.js"></script>


<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
plugins: [RevealMarkdown,RevealZoom,RevealNotes],
slideNumber:'c/t',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
]
});


</script>
</body>
</html>
